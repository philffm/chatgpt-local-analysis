{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "# !conda install -c conda-forge plyvel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# ChatGPT analysis\n",
       "\n",
       "In this notebook I deliver the foundation to analyse the personal ChatGPT conversations. After a few months of using ChatGPT, I have collected a large amount of data and want to analyse this data to get a better understanding of my own behaviour, reflect on old conversations and spark ideas on how to use it in the future.\n",
       "\n",
       "Using the Superpower ChatGPT extension for Chrome you can automatically sync your conversations for offline usage. Since all your conversations are now stored locally, you can analyse the local database to get insights into your conversations.\n",
       "\n",
       "## Introduction\n",
       "\n",
       "\n",
       "- Superpower ChatGPT extension: https://github.com/saeedezzati/superpower-chatgpt\n",
       "\n",
       "\n",
       "## Current state\n",
       "\n",
       "- [x] Get data from local database to a pandas dataframe\n",
       "    - [x] df_conversations\n",
       "    - [x] df_messages\n",
       "\n",
       "- [x] Streamlint Dashboard\n",
       "    - [x] Conversation overview\n",
       "    - [x] Message overview\n",
       "    - [x] Message analysis\n",
       "\n",
       "## Pull requests\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import readme.md file (project overview) and display the markdown file here\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "with open('README.md', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "display(Markdown(content))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plyvel\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data from browser database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 106048\n",
      "-rw-------@ 1 phil  staff   7720330 May 30 17:41 003033.ldb\n",
      "-rw-------@ 1 phil  staff    131157 Jun  9 16:46 006828.ldb\n",
      "-rw-------@ 1 phil  staff   7920197 Jun  9 22:35 007012.ldb\n",
      "-rw-------@ 1 phil  staff    129191 Jun  9 22:35 007013.ldb\n",
      "-rw-------@ 1 phil  staff   7882321 Jun  9 22:35 007015.ldb\n",
      "-rw-------@ 1 phil  staff  22339962 Jun  9 22:36 007016.log\n",
      "-rw-------@ 1 phil  staff   7755106 Jun  9 22:36 007017.ldb\n",
      "-rw-------@ 1 phil  staff        16 May 28 21:05 CURRENT\n",
      "-rw-------@ 1 phil  staff         0 May 28 21:05 LOCK\n",
      "-rw-------@ 1 phil  staff     65298 Jun  9 22:36 LOG\n",
      "-rw-------@ 1 phil  staff      1774 Jun  8 18:10 LOG.old\n",
      "-rw-------@ 1 phil  staff    318794 Jun  9 22:36 MANIFEST-000001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to the leveldb directory (change this to your path - in my case I am using Brave Browser)\n",
    "leveldb_path = '~/Library/Application Support/BraveSoftware/Brave-Browser/Default/Local Extension Settings/amhmeenmapldpjdedekalnfifgnpfnkc'\n",
    "leveldb_path = os.path.expanduser(leveldb_path)  # Expand the '~' symbol to the user's home directory\n",
    "\n",
    "# Enclose the path in quotes to handle spaces\n",
    "leveldb_path = f'\"{leveldb_path}\"'\n",
    "\n",
    "# Get the list of files in the leveldb directory\n",
    "\n",
    "!ls -l $leveldb_path\n",
    "\n",
    "# Copy all files to the current directory /db\n",
    "\n",
    "!cp -r $leveldb_path ./db\n",
    "\n",
    "# set leveldb path to the copied directory\n",
    "\n",
    "leveldb_path = './db/amhmeenmapldpjdedekalnfifgnpfnkc'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'                               Compactions\\nLevel  Files Size(MB) Time(sec) Read(MB) Write(MB)\\n--------------------------------------------------\\n  0        3       22         0        0         7\\n  1        2        8         0        0         0\\n  2        2        7         0        0         0\\n'\n"
     ]
    }
   ],
   "source": [
    "db = plyvel.DB(leveldb_path)\n",
    "\n",
    "# get database info\n",
    "\n",
    "print(db.get_property(b'leveldb.stats'))\n",
    "\n",
    "\n",
    "# filter all key entries that contain chat.openai.com \n",
    "\n",
    "for key, value in db:\n",
    "    if b'chat.openai.com' in key:\n",
    "        print(f'Key: {key}, Value: {value}')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(columns=['key', 'value'])\n",
    "\n",
    "for key, value in db:\n",
    "    df = pd.concat([df, pd.DataFrame({'key': [key], 'value': [value]})], ignore_index=True)\n",
    "    # df = df.append({'key': key, 'value': value}, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the value of key \"conversations\"  as json in a separate dataframe\n",
    "\n",
    "df_conversations = pd.DataFrame(json.loads(df[df['key'] == b'conversations']['value'].values[0]))\n",
    "# with column / lines transposed\n",
    "df_conversations = df_conversations.T\n",
    "# also export as text file\n",
    "df_conversations.to_csv('df_conversations.txt', sep='\\t', index=False)\n",
    "\n",
    "# # add a column for the key value length\n",
    "df['key_length'] = df['value'].str.len()\n",
    "\n",
    "df.head()\n",
    "\n",
    "# close the database\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop all rows of df_conversations with empty mapping\n",
    "df_conversations = df_conversations[df_conversations['mapping'].str.len() > 0]\n",
    "\n",
    "# Create a new DataFrame for messages\n",
    "df_messages = pd.DataFrame(columns=['id','conversation_id', 'title', 'message', 'role', 'create_time'])\n",
    "\n",
    "# Iterate over the conversations and messages\n",
    "for idx, conversation in df_conversations.iterrows():\n",
    "    mapping = conversation['mapping']\n",
    "    title = conversation['title']\n",
    "    conversation_id = conversation['id']\n",
    "    \n",
    "    # Iterate over the messages in the mapping\n",
    "    for message_id, message_data in mapping.items():\n",
    "        if 'message' in message_data and message_data['message'] is not None:\n",
    "            message = message_data['message']\n",
    "            \n",
    "            # Get the message text\n",
    "            message_text = ''\n",
    "            if 'content' in message and 'parts' in message['content']:\n",
    "                message_parts = message['content']['parts']\n",
    "                if len(message_parts) > 0:\n",
    "                    message_text = message_parts[0]\n",
    "            \n",
    "            # Get the author role\n",
    "            role = ''\n",
    "            if 'author' in message and 'role' in message['author']:\n",
    "                role = message['author']['role']\n",
    "            \n",
    "            # Get the create time\n",
    "            create_time = ''\n",
    "            if 'create_time' in message:\n",
    "                create_time = message['create_time']\n",
    "            \n",
    "            # Append the message data to the DataFrame\n",
    "            # df_messages = df_messages.append({\n",
    "            #     'id': message_id,\n",
    "            #     'title': title,\n",
    "            #     'message': message_text,\n",
    "            #     'role': role,\n",
    "            #     'create_time': create_time\n",
    "            # }, ignore_index=True)\n",
    "\n",
    "            df_messages = pd.concat([df_messages, pd.DataFrame({\n",
    "                'id': [message_id],\n",
    "                'conversation_id': [conversation_id],\n",
    "                'title': [title],\n",
    "                'message': [message_text],\n",
    "                'role': [role],\n",
    "                'create_time': [create_time]\n",
    "                })], ignore_index=True)\n",
    "            \n",
    "\n",
    "\n",
    "# Close the LevelDB database\n",
    "db.close()\n",
    "\n",
    "# Display the first few rows of the messages DataFrame\n",
    "print(df_messages.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/phil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/phil/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/var/folders/wc/1fgpcwpx4rzfbbc7shb_wy8h0000gn/T/ipykernel_10820/1433106567.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_messages['cleaned_text'] = df_messages['cleaned_text'].str.replace('[^\\w\\s]','')\n",
      "/var/folders/wc/1fgpcwpx4rzfbbc7shb_wy8h0000gn/T/ipykernel_10820/1433106567.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_messages['cleaned_text'] = df_messages['cleaned_text'].str.replace('[\\d+]','')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('div', 11351), ('px', 7869), ('color', 3109), ('null', 2996), ('file', 2646), ('template', 2567), ('da', 2412), ('false', 2331), ('value', 2319), ('script', 2258)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# add more stop words in english and german\n",
    "stop_words = stopwords.words('english')\n",
    "# also exclude code words like < >, the, to, and =, a, in , of, for [ ] ( ) {} etc.\n",
    "code_words = ['<', '>', 'the', 'to', 'and', '=', 'a', 'in', 'of', 'for', '[', ']', '(', ')', '{', '}', 'const', 'import', 'script', 'button', 'await', 'null', 'code', 'div', 'und', 'px', 'data', 'file', 'die', 'return', 'image', 'user', 'der', 'use', 'error', 'value', 'new', 'color', 'zu', 'create', 'using', 'component', 'add', 'false', 'object', 'template', 'name', 'da', 'also', 'app', 'example', 'span', 'für', 'width', 'mit', 'type', 'content', 'label', 'method', 'display', 'feedback', 'bike', 'rating', 'style', 'location', 'e', 'backgroundcolor', 'try', 'height', 'center', 'button', 'title', 'div', 'px', 'color', 'null', 'file', 'template', 'da', 'false', 'value', 'script', 'span', 'error', 'backgroundcolor', 'e', 'button', 'sie', 'ist', 'true', 'cycling', 'make', 'eine', 'class', 'default', 'auf', 'could', 'von', 'heres', 'like', 'name', 'function', 'used', 'i', 'need', 'async', 'based', 'label', 'data', 'advice', 'style', 'center', 'submission', 'infrastructure', 'set', 'model', 'property', 'width', 'type', 'id', 'ein', 'report', 'div', 'px', 'color', 'null', 'file', 'template', 'da', 'false', 'value', 'script', 'span', 'error', 'backgroundcolor', 'e', 'button', 'heres', 'name', 'i', 'label', 'data', 'style', 'center', 'width', 'type', 'index', 'text', 'vue', 'map', 'get', 'den', 'title', 'image', 'user', 'issue', 'array', 'f', 'column', 'comment', 'element', 'true', 'list', 'display', 'p', 'sure', 'im', 'result', 'height']\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df_messages['cleaned_text'] = df_messages['message'].str.lower()\n",
    "# strip out words from stop_words and code_words from cleaned_text column\n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words + code_words)]))\n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].str.replace('[^\\w\\s]','')\n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].str.replace('[\\d+]','')\n",
    "\n",
    "df_messages.head()\n",
    "\n",
    "# based on df_messages['cleaned_text'] column create word frequency count \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Create a list of all words in the messages\n",
    "all_words = []\n",
    "\n",
    "for idx, row in df_messages.iterrows():\n",
    "    all_words.extend(row['cleaned_text'].split())\n",
    "\n",
    "# Create a word frequency counter\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Display the 10 most common words\n",
    "\n",
    "print(word_freq.most_common(10))\n",
    "\n",
    "# Create a list of all words in the messages\n",
    "\n",
    "all_words = []\n",
    "\n",
    "# save the messages dataframe as txt\n",
    "\n",
    "df_messages.to_csv('df_messages.txt', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pyvis and jinja2\n",
    "# !pip install pyvis\n",
    "# !pip install jinja2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard\n",
    "\n",
    "Initialize the streamlint dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install streamlit\n",
    "# !pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install worldcloud\n",
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install networkx\n",
    "# %pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run streamlit app\n",
    "\n",
    "code = \"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# read custom_stop_words from txt file\n",
    "\n",
    "custom_stop_words = []\n",
    "with open('custom_stop_words.txt', 'r') as f:\n",
    "    custom_stop_words = f.read().splitlines()\n",
    "\n",
    "\n",
    "# Read the chat logs data into a DataFrame\n",
    "df_messages = pd.read_csv('df_messages.txt', sep='\t')\n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].astype(str)  # Convert 'cleaned_text' column to string type\n",
    "# filter out code words in cleaned_text column \n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in custom_stop_words]))\n",
    "\n",
    "# Process the chat messages to extract words\n",
    "all_words = ' '.join(df_messages['cleaned_text']).lower().split()\n",
    "all_words = [word for word in all_words if word not in custom_stop_words]\n",
    "word_counts = Counter(all_words)\n",
    "most_common_words = word_counts.most_common(200)  # Change the number as per your requirement\n",
    "\n",
    "# Create a DataFrame for the most common words\n",
    "df_common_words = pd.DataFrame(most_common_words, columns=['Word', 'Count'])\n",
    "\n",
    "# Display the most common words in a bar chart\n",
    "fig, ax = plt.subplots()\n",
    "df_common_words.plot.bar(x='Word', y='Count', ax=ax)\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Most Common Words in Chat Logs')\n",
    "plt.xticks(rotation=45)\n",
    "st.pyplot(fig)\n",
    "\n",
    "# Display the raw data of the most common words\n",
    "st.write(df_common_words)\n",
    "\n",
    "# Display the raw data of the chat logs\n",
    "st.write(df_messages)\n",
    "\n",
    "# display a word cloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create a word cloud\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200).generate(' '.join(df_messages['cleaned_text']))\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.set_axis_off()\n",
    "st.pyplot(fig)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to the dashboard.py file\n",
    "with open('dashboard.py', 'w') as f:\n",
    "    f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8502\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.168.178.60:8502\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \u001b[0m\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# run streamlit app dashboard.py\n",
    "!streamlit run dashboard.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# close the database\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_notebook_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
